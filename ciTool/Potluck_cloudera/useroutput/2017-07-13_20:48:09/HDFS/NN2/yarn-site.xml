<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>mapreduce.shuffle.port</name>
        <value>23080</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>30720</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
        <description>Disable because it cause issue w/ gvs hadoop + distcp</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>106496</value>
    </property>
    <property>
       <name>yarn.nodemanager.vmem-pmem-ratio</name>
       <value>2.1</value>
    </property>
    
    <property>
       <description>Frequency of running disk health checker code.</description>
       <name>yarn.nodemanager.disk-health-checker.interval-ms</name>
       <value>120000</value>
    </property>
    <property>
        <description>The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers. This correspond to both yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.</description>
        <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
        <value>0.25</value>
    </property>
    <property>
        <description>The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs.</description>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>100</value>
    </property>
    <property>
       <description>The minimum space that must be available on a disk for it to be used. This applies to yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs.</description>
       <name>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb</name>
       <value>0</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>9</value>
        <description>Number of CPU cores that can be allocated for containers.</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>8</value>
        <description>The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won't take effect, and will get capped to this value.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.nodes.exclude-path</name>
        <value>/etc/hadoop/conf/yarn.exclude</value>
        <description>Path to file with nodes to exclude.</description>
    </property>

    <property>
        <description>Classpath for typical applications.</description>
        <name>yarn.application.classpath</name>
        <value>$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,$YARN_HOME/*,$YARN_HOME/lib/*</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>file:///opt/data01/yarn-local-dir</value>
    </property>
    <property>
        <description>NM Webapp address.</description>
        <name>yarn.nodemanager.webapp.address</name>
        <value>0.0.0.0:8042</value>
    </property>
    <property>
        <description>Address where the localizer IPC is.</description>
        <name>yarn.nodemanager.localizer.address</name>
        <value>0.0.0.0:8040</value>
    </property>
    <property>
        <name>yarn.nodemanager.delete.debug-delay-sec</name>
        <value>21600</value>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>file:///opt/yarn/logs</value>
    </property>
    <property>
        <description>Where to aggregate logs</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://marxnn1.guavus.com:19888/jobhistory/logs/</value>
        <description>URL for job history server</description>
    </property>
<!-- HA Resource Manager Configs START Created when deployment_type flag == HA -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <!--Automatic Failover Configs START-->
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>srx</value>
    </property>
    <!-- Automatic Failover Configs END -->
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>marxnn1.guavus.com,marxnn2.guavus.com</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.id</name>
        <value>marxnn2.guavus.com</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com</value>
    </property>
    <property>
        <name>yarn.resourcemanager.connect.retry-interval.ms</name>
        <value>2000</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>marxdn1.guavus.com:2181,marxdn2.guavus.com:2181,marxdn3.guavus.com:2181</value>
        <description>For multiple zk services, separate them with comma</description>
    </property>
 
   <!--RM1  host configs-->
    <property>
        <name>yarn.resourcemanager.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.https.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8090</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.marxnn1.guavus.com</name>
        <value>marxnn1.guavus.com:8033</value>
    </property> 
    <!--RM2  host configs-->
    <property>
        <name>yarn.resourcemanager.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.https.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8090</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.marxnn2.guavus.com</name>
        <value>marxnn2.guavus.com:8033</value>
    </property>
</configuration>
